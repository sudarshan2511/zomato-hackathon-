{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 5 — GRU Cart Encoder + LightGBM Multi-Objective Ranker\n",
        "\n",
        "**Two components working together:**\n",
        "1. **GRU Cart Encoder** (PyTorch) — processes cart items in addition order → 64-dim trajectory vector\n",
        "2. **LightGBM Ranker** — five models, each optimising one business objective\n",
        "\n",
        "**Final score:** `0.30·Accept + 0.30·AOV − 0.20·Abandon + 0.10·Timing + 0.10·Anchor`\n",
        "\n",
        "| Model | Predicts | Objective | Label |\n",
        "|-------|----------|-----------|-------|\n",
        "| Accept | P(user adds item) | LambdaRank | Was item added? |\n",
        "| AOV | Value contribution | Regression | price × accepted / 500 |\n",
        "| Abandon | P(cart abandoned) | Binary | Session ended without order? |\n",
        "| Timing | P(fits cart stage) | Binary | Accepted at appropriate stage? |\n",
        "| Anchor | P(position-1 item) | Binary | Accepted when shown at pos 1? |"
      ],
      "id": "ca6db608"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys, os\n",
        "sys.path.insert(0, os.path.abspath(\"..\"))\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "pd.set_option(\"display.max_columns\", 80)\n",
        "pd.set_option(\"display.width\", 200)\n",
        "\n",
        "DATA_DIR  = os.path.abspath(\"../data\")\n",
        "MODEL_DIR = os.path.abspath(\"../models\")\n",
        "\n",
        "print(\"Imports OK\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "f1196cf9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data"
      ],
      "id": "540a50b1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "features  = pd.read_csv(f\"{DATA_DIR}/training_features.csv\")\n",
        "sessions  = pd.read_csv(f\"{DATA_DIR}/sessions.csv\")\n",
        "cart_events = pd.read_csv(f\"{DATA_DIR}/cart_events.csv\")\n",
        "\n",
        "print(f\"Training features : {features.shape[0]:,} rows × {features.shape[1]} cols\")\n",
        "print(f\"Sessions          : {len(sessions):,}\")\n",
        "print(f\"Cart events       : {len(cart_events):,}\")\n",
        "print(f\"\\nLabel distribution:\")\n",
        "print(f\"  label_accept       : {features['label_accept'].mean():.3f} positive rate\")\n",
        "print(f\"  label_cart_abandoned: {features['label_cart_abandoned'].mean():.3f} positive rate\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "b1bf46a1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. GRU Cart Encoder — Training\n",
        "\n",
        "The GRU processes cart items in the exact order they were added. A cart of **Biryani → Salan** encodes differently than **Salan alone** — capturing the evolving meal trajectory that mean-pooling cannot."
      ],
      "id": "021b8850"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from gru_encoder import (\n",
        "    load_embeddings, build_cart_sequences, train_gru,\n",
        "    extract_hidden_states, CartGRUEncoder\n",
        ")\n",
        "import torch\n",
        "\n",
        "emb_lookup = load_embeddings(f\"{DATA_DIR}/item_embeddings.npz\")\n",
        "emb_dim = next(iter(emb_lookup.values())).shape[0]\n",
        "print(f\"Loaded {len(emb_lookup):,} item embeddings (dim={emb_dim})\")\n",
        "\n",
        "print(\"\\nBuilding cart sequences for each recommendation impression …\")\n",
        "sequences, candidate_embs, labels = build_cart_sequences(\n",
        "    features, cart_events, emb_lookup, emb_dim\n",
        ")\n",
        "print(f\"Built {len(sequences):,} sequences\")\n",
        "print(f\"Sequence length stats: min={min(len(s) for s in sequences)}, \"\n",
        "      f\"max={max(len(s) for s in sequences)}, \"\n",
        "      f\"mean={np.mean([len(s) for s in sequences]):.1f}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "04990eb9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Temporal split for GRU training\n",
        "sessions[\"start_time\"] = pd.to_datetime(sessions[\"start_time\"])\n",
        "sess_start = sessions.set_index(\"session_id\")[\"start_time\"]\n",
        "feat_sess_start = features[\"session_id\"].map(sess_start)\n",
        "\n",
        "train_mask_gru = (feat_sess_start < \"2025-12-22\").values\n",
        "val_mask_gru   = ((feat_sess_start >= \"2025-12-22\") & (feat_sess_start < \"2025-12-29\")).values\n",
        "\n",
        "print(f\"GRU train: {train_mask_gru.sum():,}  |  GRU val: {val_mask_gru.sum():,}\")\n",
        "\n",
        "print(\"\\nTraining GRU Cart Encoder …\")\n",
        "gru_model, gru_history = train_gru(\n",
        "    sequences, candidate_embs, labels,\n",
        "    train_mask_gru, val_mask_gru,\n",
        "    input_dim=emb_dim, hidden_dim=64,\n",
        "    batch_size=256, lr=1e-3, epochs=20, patience=3,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "f79dffbe"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# GRU training loss curve\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "epochs_range = range(1, len(gru_history[\"train_loss\"]) + 1)\n",
        "ax.plot(epochs_range, gru_history[\"train_loss\"], \"o-\", label=\"Train Loss\")\n",
        "ax.plot(epochs_range, gru_history[\"val_loss\"], \"s-\", label=\"Val Loss\")\n",
        "ax.set_xlabel(\"Epoch\")\n",
        "ax.set_ylabel(\"BCE Loss\")\n",
        "ax.set_title(\"GRU Cart Encoder — Training Curve\")\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "best_epoch = np.argmin(gru_history[\"val_loss\"]) + 1\n",
        "print(f\"Best epoch: {best_epoch}  (val_loss={min(gru_history['val_loss']):.4f})\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "21d45182"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Extract GRU Hidden States\n",
        "\n",
        "Run the trained GRU on all sequences to produce 64-dim trajectory features for LightGBM."
      ],
      "id": "862d6d69"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Extracting 64-dim hidden states …\")\n",
        "gru_hidden = extract_hidden_states(gru_model, sequences, batch_size=512)\n",
        "print(f\"Shape: {gru_hidden.shape}\")\n",
        "\n",
        "# Save GRU artifacts\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "torch.save(gru_model.state_dict(), f\"{MODEL_DIR}/gru_encoder.pt\")\n",
        "np.save(f\"{DATA_DIR}/gru_hidden_states.npy\", gru_hidden)\n",
        "print(f\"GRU weights saved  → {MODEL_DIR}/gru_encoder.pt\")\n",
        "print(f\"Hidden states saved → {DATA_DIR}/gru_hidden_states.npy\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "b88dd506"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Label Engineering + Feature Preparation"
      ],
      "id": "48446e54"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from lgbm_ranker import (\n",
        "    engineer_labels, prepare_features, temporal_split,\n",
        "    train_lgbm_models, tune_hyperparameters,\n",
        "    evaluate_models, evaluate_by_segment, evaluate_by_cart_stage,\n",
        "    evaluate_cold_start, compute_business_score,\n",
        "    shap_analysis, save_models, MODEL_CONFIGS,\n",
        ")\n",
        "\n",
        "# Engineer all 5 label targets\n",
        "features = engineer_labels(features)\n",
        "\n",
        "label_cols = [\"label_accept\", \"label_aov\", \"label_cart_abandoned\", \"label_timing\", \"label_anchor\"]\n",
        "print(\"Label statistics:\")\n",
        "for col in label_cols:\n",
        "    print(f\"  {col:25s}  mean={features[col].mean():.4f}  \"\n",
        "          f\"sum={features[col].sum():,.0f}  \"\n",
        "          f\"(positive rate {100*features[col].mean():.1f}%)\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "aa64b730"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prepare features (append GRU hidden states, encode categoricals)\n",
        "features, feature_cols, encoders = prepare_features(features, gru_hidden)\n",
        "print(f\"Total feature columns: {len(feature_cols)}\")\n",
        "\n",
        "# Temporal split\n",
        "train_mask, val_mask, test_mask = temporal_split(features, sessions)\n",
        "print(f\"\\nTemporal split:\")\n",
        "print(f\"  Train (weeks 49-51): {train_mask.sum():,} rows\")\n",
        "print(f\"  Val   (week 52)    : {val_mask.sum():,} rows\")\n",
        "print(f\"  Test  (weeks 1-2)  : {test_mask.sum():,} rows\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "ea738426"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. MBA Baseline\n",
        "\n",
        "Market Basket Analysis (Apriori + association rules) — the industry-standard baseline. Its failures motivate every design decision in our main system."
      ],
      "id": "25a02680"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from mba_baseline import build_transaction_baskets, train_mba, evaluate_mba\n",
        "\n",
        "# Build baskets from training-period completed sessions only\n",
        "train_sessions = sessions[sessions[\"start_time\"] < \"2025-12-22\"]\n",
        "baskets = build_transaction_baskets(cart_events, train_sessions)\n",
        "print(f\"Transaction baskets: {len(baskets):,}\")\n",
        "\n",
        "# Run Apriori\n",
        "mba_rules = train_mba(baskets, min_support=0.003, min_confidence=0.05)\n",
        "if len(mba_rules) > 0:\n",
        "    print(f\"\\nTop 10 rules by lift:\")\n",
        "    display(mba_rules.nlargest(10, \"lift\")[\n",
        "        [\"antecedents\", \"consequents\", \"support\", \"confidence\", \"lift\"]\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "5a1ca9f7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Evaluate MBA on test set\n",
        "test_features_df = features[test_mask]\n",
        "mba_metrics = evaluate_mba(mba_rules, test_features_df, cart_events, k=8)\n",
        "print(\"MBA Baseline — Test Set Metrics:\")\n",
        "for k, v in mba_metrics.items():\n",
        "    print(f\"  {k:20s}: {v:.4f}\" if isinstance(v, float) else f\"  {k:20s}: {v}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "27bc7078"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. LightGBM Training — Five Models\n",
        "\n",
        "Training all five models with default hyperparameters, then tuning."
      ],
      "id": "2ad32e89"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"TRAINING INITIAL MODELS (default hyperparameters)\")\n",
        "print(\"=\" * 60)\n",
        "models_initial = train_lgbm_models(features, feature_cols, train_mask, val_mask)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "48492b9b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Hyperparameter Tuning\n",
        "\n",
        "Grid search over `num_leaves`, `learning_rate`, `min_child_samples` with early stopping."
      ],
      "id": "6326c4c0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"HYPERPARAMETER TUNING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "best_params_all = {}\n",
        "tuning_results = {}\n",
        "\n",
        "for model_name in MODEL_CONFIGS:\n",
        "    bp, results_df = tune_hyperparameters(\n",
        "        features, feature_cols, train_mask, val_mask, model_name=model_name\n",
        "    )\n",
        "    best_params_all[model_name] = bp\n",
        "    tuning_results[model_name] = results_df\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Best hyperparameters per model:\")\n",
        "for name, params in best_params_all.items():\n",
        "    print(f\"  [{name:8s}] {params}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "b2d6e1c8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Retrain with best hyperparameters\n",
        "print(\"=\" * 60)\n",
        "print(\"RETRAINING WITH TUNED HYPERPARAMETERS\")\n",
        "print(\"=\" * 60)\n",
        "models = train_lgbm_models(\n",
        "    features, feature_cols, train_mask, val_mask,\n",
        "    params_override=best_params_all,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "141c4b88"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluation — All Models on Test Set"
      ],
      "id": "eaee86ab"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"TEST SET EVALUATION\")\n",
        "print(\"=\" * 60)\n",
        "test_metrics = evaluate_models(models, features, feature_cols, test_mask)\n",
        "\n",
        "print(\"\\nPer-model metrics:\")\n",
        "for name, m in test_metrics.items():\n",
        "    parts = \"  \".join(f\"{k}={v:.4f}\" for k, v in m.items())\n",
        "    print(f\"  [{name:8s}] {parts}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "5bf199a8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Our System vs MBA Baseline — Head-to-Head Comparison"
      ],
      "id": "41a45fcd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Side-by-side comparison\n",
        "our_p8 = test_metrics[\"accept\"].get(\"precision_at_8\", 0)\n",
        "our_ndcg = test_metrics[\"accept\"].get(\"ndcg_at_8\", 0)\n",
        "our_auc = test_metrics[\"accept\"].get(\"auc_roc\", 0)\n",
        "mba_p8 = mba_metrics.get(\"precision_at_k\", 0)\n",
        "mba_ndcg = mba_metrics.get(\"ndcg_at_k\", 0)\n",
        "\n",
        "comparison = pd.DataFrame({\n",
        "    \"Metric\": [\"Precision@8\", \"NDCG@8\", \"AUC-ROC\"],\n",
        "    \"MBA Baseline\": [mba_p8, mba_ndcg, \"N/A\"],\n",
        "    \"Our System\": [our_p8, our_ndcg, our_auc],\n",
        "    \"Improvement\": [\n",
        "        f\"+{100*(our_p8 - mba_p8)/max(mba_p8, 1e-6):.1f}%\" if mba_p8 > 0 else \"N/A (MBA=0)\",\n",
        "        f\"+{100*(our_ndcg - mba_ndcg)/max(mba_ndcg, 1e-6):.1f}%\" if mba_ndcg > 0 else \"N/A (MBA=0)\",\n",
        "        \"—\"\n",
        "    ],\n",
        "})\n",
        "display(comparison)\n",
        "\n",
        "# Bar chart\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "for ax, metric, our_val, mba_val in [\n",
        "    (axes[0], \"Precision@8\", our_p8, mba_p8),\n",
        "    (axes[1], \"NDCG@8\", our_ndcg, mba_ndcg),\n",
        "]:\n",
        "    bars = ax.bar([\"MBA Baseline\", \"Our System\"], [mba_val, our_val],\n",
        "                  color=[\"#95a5a6\", \"#e74c3c\"], edgecolor=\"black\", linewidth=0.5)\n",
        "    ax.set_title(metric, fontsize=14, fontweight=\"bold\")\n",
        "    ax.set_ylim(0, max(our_val, mba_val) * 1.3 + 0.01)\n",
        "    for bar in bars:\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
        "                f\"{bar.get_height():.4f}\", ha=\"center\", fontsize=11)\n",
        "plt.suptitle(\"MBA Baseline vs GRU+LightGBM System\", fontsize=15, fontweight=\"bold\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "2dc3df81"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Breakdown — By User Segment and Cart Stage\n",
        "\n",
        "The system should outperform MBA most at higher cart stages (2-3), where sequential understanding matters most."
      ],
      "id": "433301fa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# By user segment\n",
        "print(\"Accept Model — Precision@8 & NDCG@8 by User Segment\")\n",
        "print(\"-\" * 55)\n",
        "seg_df = evaluate_by_segment(models, features, feature_cols, test_mask, encoders=encoders)\n",
        "display(seg_df)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "seg_df.plot.bar(x=\"segment\", y=\"precision_at_8\", ax=axes[0], color=\"#3498db\",\n",
        "                legend=False, edgecolor=\"black\", linewidth=0.5)\n",
        "axes[0].set_title(\"Precision@8 by Segment\", fontweight=\"bold\")\n",
        "axes[0].set_ylabel(\"Precision@8\")\n",
        "axes[0].tick_params(axis=\"x\", rotation=30)\n",
        "\n",
        "seg_df.plot.bar(x=\"segment\", y=\"ndcg_at_8\", ax=axes[1], color=\"#2ecc71\",\n",
        "                legend=False, edgecolor=\"black\", linewidth=0.5)\n",
        "axes[1].set_title(\"NDCG@8 by Segment\", fontweight=\"bold\")\n",
        "axes[1].set_ylabel(\"NDCG@8\")\n",
        "axes[1].tick_params(axis=\"x\", rotation=30)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "76395e1a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# By cart stage\n",
        "print(\"Accept Model — Precision@8 & NDCG@8 by Cart Stage\")\n",
        "print(\"-\" * 55)\n",
        "stage_df = evaluate_by_cart_stage(models, features, feature_cols, test_mask)\n",
        "display(stage_df)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "stage_df.plot.bar(x=\"cart_stage\", y=\"precision_at_8\", ax=axes[0], color=\"#e67e22\",\n",
        "                  legend=False, edgecolor=\"black\", linewidth=0.5)\n",
        "axes[0].set_title(\"Precision@8 by Cart Stage\", fontweight=\"bold\")\n",
        "axes[0].set_xlabel(\"Cart Stage (0=empty, 3=nearly complete)\")\n",
        "\n",
        "stage_df.plot.bar(x=\"cart_stage\", y=\"ndcg_at_8\", ax=axes[1], color=\"#9b59b6\",\n",
        "                  legend=False, edgecolor=\"black\", linewidth=0.5)\n",
        "axes[1].set_title(\"NDCG@8 by Cart Stage\", fontweight=\"bold\")\n",
        "axes[1].set_xlabel(\"Cart Stage (0=empty, 3=nearly complete)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "4f8e41af"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Cold Start Evaluation\n",
        "\n",
        "Testing on low-history users (≤ 5 orders). The system uses `bestseller_flag` + `popularity_score` as fallback signals where user history is sparse — something MBA cannot do."
      ],
      "id": "0980da05"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cold_results = evaluate_cold_start(\n",
        "    models, features, feature_cols, test_mask,\n",
        "    order_count_col=\"user_order_count\", cold_threshold=5,\n",
        ")\n",
        "\n",
        "print(\"Cold Start vs Warm Users:\")\n",
        "print(\"-\" * 55)\n",
        "cold_df = pd.DataFrame(cold_results).T\n",
        "cold_df.index.name = \"user_type\"\n",
        "display(cold_df)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "x = np.arange(2)\n",
        "width = 0.35\n",
        "cold_vals = [cold_results[\"cold\"][\"precision_at_8\"], cold_results[\"cold\"][\"ndcg_at_8\"]]\n",
        "warm_vals = [cold_results[\"warm\"][\"precision_at_8\"], cold_results[\"warm\"][\"ndcg_at_8\"]]\n",
        "ax.bar(x - width/2, cold_vals, width, label=f\"Cold (≤5 orders, n={cold_results['cold']['sessions']:.0f})\",\n",
        "       color=\"#e74c3c\", edgecolor=\"black\", linewidth=0.5)\n",
        "ax.bar(x + width/2, warm_vals, width, label=f\"Warm (>5 orders, n={cold_results['warm']['sessions']:.0f})\",\n",
        "       color=\"#2ecc71\", edgecolor=\"black\", linewidth=0.5)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels([\"Precision@8\", \"NDCG@8\"])\n",
        "ax.set_title(\"Cold Start vs Warm Users — Accept Model\", fontweight=\"bold\")\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "ae8eaa03"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Business Score — Final Ranking with Peak-Hour Switching"
      ],
      "id": "53b3a3ed"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_test = features.loc[test_mask, feature_cols]\n",
        "preds = {name: model.predict(X_test) for name, model in models.items()}\n",
        "\n",
        "# Default weights\n",
        "default_scores = compute_business_score(preds)\n",
        "\n",
        "# Peak-hour adaptive weights\n",
        "peak_modes = features.loc[test_mask, \"peak_hour_mode\"].values\n",
        "# Decode back to string labels\n",
        "if \"peak_hour_mode\" in encoders:\n",
        "    peak_str = encoders[\"peak_hour_mode\"].inverse_transform(peak_modes.astype(int))\n",
        "else:\n",
        "    peak_str = peak_modes\n",
        "adaptive_scores = compute_business_score(preds, peak_mode=peak_str)\n",
        "\n",
        "print(\"Business Score Statistics (test set):\")\n",
        "print(f\"  Default weights   — mean={default_scores.mean():.4f}  std={default_scores.std():.4f}\")\n",
        "print(f\"  Peak-hour adaptive — mean={adaptive_scores.mean():.4f}  std={adaptive_scores.std():.4f}\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "axes[0].hist(default_scores, bins=50, color=\"#3498db\", edgecolor=\"black\", alpha=0.7)\n",
        "axes[0].set_title(\"Business Score Distribution (Default Weights)\", fontweight=\"bold\")\n",
        "axes[0].set_xlabel(\"Score\")\n",
        "axes[1].hist(adaptive_scores, bins=50, color=\"#e74c3c\", edgecolor=\"black\", alpha=0.7)\n",
        "axes[1].set_title(\"Business Score Distribution (Peak-Hour Adaptive)\", fontweight=\"bold\")\n",
        "axes[1].set_xlabel(\"Score\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "from lgbm_ranker import DEFAULT_WEIGHTS, PEAK_WEIGHTS\n",
        "print(\"\\nWeight configurations:\")\n",
        "print(f\"  Default: {DEFAULT_WEIGHTS}\")\n",
        "for mode, w in PEAK_WEIGHTS.items():\n",
        "    print(f\"  {mode}: {w}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "6bc14e5e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. SHAP Feature Importance\n",
        "\n",
        "Which features drive each model's recommendations? Judges can see exactly what matters."
      ],
      "id": "3486cca6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathlib import Path\n",
        "\n",
        "shap_results = shap_analysis(\n",
        "    models, features, feature_cols, test_mask,\n",
        "    save_dir=Path(MODEL_DIR), max_samples=2000,\n",
        ")\n",
        "\n",
        "# Display saved SHAP plots inline\n",
        "from IPython.display import Image, display as ipy_display\n",
        "for name in MODEL_CONFIGS:\n",
        "    plot_path = f\"{MODEL_DIR}/shap_{name}.png\"\n",
        "    if os.path.exists(plot_path):\n",
        "        print(f\"\\n{'=' * 40}\")\n",
        "        print(f\"SHAP — {name.upper()} Model\")\n",
        "        print(f\"{'=' * 40}\")\n",
        "        ipy_display(Image(filename=plot_path, width=700))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "235232d5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Save All Artifacts"
      ],
      "id": "ac9fd2f9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Save LightGBM models\n",
        "save_models(models, save_dir=Path(MODEL_DIR))\n",
        "\n",
        "# Save evaluation summary\n",
        "import json\n",
        "\n",
        "eval_summary = {\n",
        "    \"test_metrics\": {k: {mk: float(mv) if isinstance(mv, (float, np.floating)) else mv\n",
        "                         for mk, mv in v.items()} for k, v in test_metrics.items()},\n",
        "    \"mba_metrics\": {k: float(v) if isinstance(v, (float, np.floating)) else v\n",
        "                    for k, v in mba_metrics.items()},\n",
        "    \"cold_start\": {k: {mk: float(mv) if isinstance(mv, (float, np.floating)) else mv\n",
        "                       for mk, mv in v.items()} for k, v in cold_results.items()},\n",
        "    \"best_hyperparameters\": {k: {pk: (int(pv) if isinstance(pv, (np.integer,)) else float(pv) if isinstance(pv, (np.floating,)) else pv)\n",
        "                                  for pk, pv in v.items()} for k, v in best_params_all.items()},\n",
        "}\n",
        "\n",
        "with open(f\"{MODEL_DIR}/eval_summary.json\", \"w\") as f:\n",
        "    json.dump(eval_summary, f, indent=2)\n",
        "\n",
        "print(\"All artifacts saved:\")\n",
        "for f_name in sorted(os.listdir(MODEL_DIR)):\n",
        "    f_path = os.path.join(MODEL_DIR, f_name)\n",
        "    size = os.path.getsize(f_path)\n",
        "    print(f\"  {f_name:35s}  {size/1024:>8.1f} KB\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "662b40bb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "**Step 5 Complete.** Trained and evaluated:\n",
        "\n",
        "1. **GRU Cart Encoder** — 64-dim trajectory features capturing sequential meal composition\n",
        "2. **5 LightGBM Models** — Accept (LambdaRank), AOV (regression), Abandon/Timing/Anchor (binary)\n",
        "3. **MBA Baseline** — Apriori association rules for comparison\n",
        "4. **Business Score** — weighted combination with peak-hour adaptive switching\n",
        "5. **SHAP Analysis** — interpretable feature importance per model\n",
        "\n",
        "**Artifacts saved to `models/`:**\n",
        "- `gru_encoder.pt` — GRU weights\n",
        "- `lgbm_accept.txt`, `lgbm_aov.txt`, `lgbm_abandon.txt`, `lgbm_timing.txt`, `lgbm_anchor.txt` — LightGBM models\n",
        "- `shap_*.png` — feature importance plots\n",
        "- `eval_summary.json` — all metrics"
      ],
      "id": "d9de3ef4"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}